# Simulation description
trial_descript: Training of 1D Chebyshev polynomial-based model
# If trial_name == 'None', then name to the experiment is given automatically,
# otherwise it is set name set by user 
trial_name: test_8

# trial_name: sgd_full_grad_adam_10k_epochs_4_pow_dim_mu_1e_3_stand_tx
# trial_name: test
# Simulation parameters
# Chebyshev polynomal order
param_num: [3, 3]
# Number of OLS iterations. Each new iteration corresponds to 1 new model branch.
iter_num: 10
batch_size: 1
# Whole signal length 213504, half 106752
block_size: 106752 # 13344
# chunk_num is important parameter, which allows to divide whole signal
# into chunk_num blocks. LS method is implemented to accumulate 
# hessian and gradient along the whole signal length and save GPU memory.
# Increase this parameter if you obtain out of memory error.
chunk_num: 1
dtype: complex128
device: "cuda:3"
# dtype: complex64
# Path to the folder with data
data_path: '../../data/data2d.mat'
# Channel to compensate: A or B
channel: A
# Allow to overwrite folder with saved results or not
overwrite_file: True

# Parameters of environment
brunch_number: 3 # delays_number = brunch_number * 3 for 2D model
delays_range: [-15, 15] # Range of delays search
max_delay_step: 1 # Max step value per single agent move
delays2change_num: 1 # Maximum number of delays to change per single agent move
max_steps: 100000 # Maximum number of PPO steps
# Normalization of state and reward parameters
state_alpha: 0.0
reward_alpha: 0.0

# Parameters of MLP agent
hidden_policy_size: 128
hidden_policy_num: 4
hidden_value_size: 128
hidden_value_num: 3
# hidden_shared_size: 128
# hidden_shared_num: 2
# kernel_shared_size: 3
# hidden_separ_size: 128
# hidden_separ_num: 1
# kernel_separ_size: 3

# General training parameters
num_runner_steps: 4096 # Steps in trajectory
gamma: 0.99 # GAE gamm
lambda_: 0.95 # GAE lambda
num_epochs_per_traj: 8 # Number of epochs per each trajectory
num_minibatches: 16 # Number of mini-batches to divide whole trajectory into
total_epoch_num: 30000 # Total number of epochs
lr_policy: 0.0002 # Adam learning rate
lr_value: 0.001 # Adam learning rate
eps: 0.00001 # Adam epsilon

# PPO training parameters
cliprange_policy: 0.2 # PPO policy clip range
cliprange_value: 0.3 # PPO value clip range
value_loss_coef: 0.35 # weight of critic loss
explore_loss_coef: 0.02 # weight of explore loss
max_grad_norm_policy: 0.5 # Maximum grad norm for clipping
max_grad_norm_value: 1 # Maximum grad norm for clipping